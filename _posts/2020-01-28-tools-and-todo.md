---
layout: post
title: tools and todos  
excerpt: "Resources I use frequently, shout out to instrumental developers in the community and projects I am currently interested in."
categories: [code,musings,tools]
modified: 2020-01-28
comments: true
---
## overview 
this is the start of a collection of tools and resources and projects currently in progress. 

## resources & links 
See [my github stars](https://github.com/xxyjoel?tab=stars) for a complete list. Here are some of my favorite:

* [public api list](https://github.com/public-apis/public-apis) - a complete list of public apis for you to get virtually any size / shape of data you could possibly want. Thank you to all the [contributors](https://github.com/public-apis/public-apis/graphs/contributors) (specifically, [Dave](https://github.com/davemachado) :smile: )! 

* [awesome python libs](https://github.com/vinta/awesome-python) - curated list of amazingly-awesome python frameworks... categorized by use case! :astonished: :muscle: :boom: Thank you to all the [contributors](https://github.com/vinta/awesome-python/graphs/contributors) (specifically, [vinta](https://github.com/vinta) :smile: )! 

* [models by industry](https://github.com/xxyjoel/industry-machine-learning) - I love when the gap between technical and business is virtually non-existent and [FirmAI](https://www.firmai.org/) does an impeccable job categorizing models by application scenarios. 


## projects
**automated eda** - while the [objectives of EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis) are to (1) suggest hypothesis about the causes of obsereved phenomena (2) assess assumptions on which statistical inference will be based (3) support the selection of appropriate statistical tools and techniques and (4) probide a basis for further data colleciton through experimentation, the techniques and tools used are fairly limited. By profiling your data prior to conducting analysis, I believe you can automate much of discovery.   

TODO - for a number of data sets, based on data types provided, size, unique value counts, missing value counts, duplicate counts, distributions and other notable attributes I am not thinking of, label all possible visuals (from the list below) and the criteria used to select them. this will set you up for a supervised learning approach. future input = pandas df, output = recommended labeled chats, graphs and summary statistics  

TODO - limitations arrise given single df input. determine how to provide same service for related tables

TODO - add / finish table of contents

TODO - transfer projects from notes to this post
